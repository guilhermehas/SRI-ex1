{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import machado, mac_morpho\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/guilherme/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package machado to\n",
      "[nltk_data]     /home/guilherme/nltk_data...\n",
      "[nltk_data]   Package machado is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('machado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = []\n",
    "for p, d, f in os.walk(r'machado/machado'):\n",
    "    #print( p,d,f)\n",
    "    if f:\n",
    "        for fileid  in f:\n",
    "            if not fileid.endswith('.txt'):\n",
    "                continue\n",
    "            with open(os.path.join(p,fileid), encoding='iso-8859-1') as g:\n",
    "                textos.append(g.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "swu = stopwords.words('portuguese') + list (string.punctuation)\n",
    "stemmer = PortugueseStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textos = textos[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_sentence(texto : str):\n",
    "    return [stemmer.stem(token.lower()) for token in WordPunctTokenizer().tokenize(texto) if token not in swu]\n",
    "\n",
    "\n",
    "def get_textos_limpos():\n",
    "    textos_limpos = []\n",
    "    for texto in textos:\n",
    "        tlimpo = clean_sentence(texto)\n",
    "        textos_limpos.append(tlimpo)\n",
    "    \n",
    "    indice = defaultdict(lambda:set([]))\n",
    "    for tid,t in enumerate(textos_limpos):\n",
    "        for term in t:\n",
    "            indice[term].add(tid)\n",
    "    return textos_limpos\n",
    "\n",
    "def get_indice_invertido():\n",
    "    textos_limpos = get_textos_limpos()\n",
    "    words_positions = {}\n",
    "    for i, sentence in enumerate(textos_limpos):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if not word in words_positions:\n",
    "                words_positions[word] = {}\n",
    "            word_d = words_positions[word]\n",
    "            if not i in word_d:\n",
    "                word_d[i] = set()\n",
    "            word_d[i].add(j)\n",
    "    \n",
    "    return words_positions\n",
    "\n",
    "indice_invertido = get_indice_invertido()\n",
    "\n",
    "def get_documents(sentence):\n",
    "    sentence = clean_sentence(sentence)\n",
    "    if not sentence:\n",
    "        return {}\n",
    "    def in_documento(doc_p):\n",
    "        def in_start(start):\n",
    "            for i, word in enumerate(sentence,start):\n",
    "                if i not in indice_invertido[word][doc_p]:\n",
    "                    return False\n",
    "            return True\n",
    "        v_pos = set()\n",
    "        for start in indice_invertido.get(sentence[0],{}).get(doc_p,[]):\n",
    "            if in_start(start):\n",
    "                v_pos.add(start)\n",
    "        return v_pos\n",
    "    docs = {}\n",
    "    for doc in indice_invertido.get(sentence[0], []):\n",
    "        docs[doc] = in_documento(doc)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "get_documents('crítric raimund')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_frequency(textos, word):\n",
    "    return sum(word in texto for texto in textos)\n",
    "\n",
    "get_frequency(get_textos_limpos(), 'critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_good_frequency(word):\n",
    "    return get_frequency(get_textos_limpos(), clean_sentence(word)[0])\n",
    "\n",
    "get_good_frequency('publicado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bad_frequency(word : str) -> int:\n",
    "    bad_text = [[token.lower() for token in WordPunctTokenizer().tokenize(texto)] for texto in textos]\n",
    "    return get_frequency(bad_text, word)\n",
    "\n",
    "get_bad_frequency('publicado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_documents_loose(sentence):\n",
    "    sentence = clean_sentence(sentence)\n",
    "    if not sentence:\n",
    "        return {}\n",
    "    def in_documento(doc_p):\n",
    "        def in_start(start):\n",
    "            last_position = [start]\n",
    "            for i, word in enumerate(sentence, last_position[0]):\n",
    "                def is_word():\n",
    "                    for position in range(i,i+4):\n",
    "                        if position in indice_invertido[word][doc_p]:\n",
    "                            last_position[0] = position\n",
    "                            return True\n",
    "                    return False\n",
    "                if not is_word():\n",
    "                    return False\n",
    "            return True\n",
    "                \n",
    "        v_pos = set()\n",
    "        for start in indice_invertido.get(sentence[0],{}).get(doc_p,[]):\n",
    "            if in_start(start):\n",
    "                v_pos.add(start)\n",
    "        return v_pos\n",
    "    docs = {}\n",
    "    for doc in indice_invertido[sentence[0]]:\n",
    "        docs[doc] = in_documento(doc)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "get_documents_loose('crítric sinfon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: {325, 395},\n",
       " 10: {1775, 6235},\n",
       " 26: {8320},\n",
       " 31: {13490},\n",
       " 43: {1897},\n",
       " 84: {1134},\n",
       " 116: {23608},\n",
       " 149: {492},\n",
       " 181: {5977},\n",
       " 192: {123},\n",
       " 199: {9289},\n",
       " 201: {5985},\n",
       " 204: {11514, 39734, 39739},\n",
       " 205: {4232},\n",
       " 209: {3482},\n",
       " 225: {1878},\n",
       " 228: {1754},\n",
       " 230: {23360, 32551, 36511, 39741, 39972, 46954, 47405, 47648},\n",
       " 232: {16106, 90084, 94966, 111437, 142999, 160411},\n",
       " 235: {19231},\n",
       " 239: {3201}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_document_from_wrong(word):\n",
    "    d = enchant.Dict(\"pt_BRL\")\n",
    "    vword = d.suggest(word) or [word]\n",
    "    word = vword[0]\n",
    "    return get_documents(word)\n",
    "\n",
    "get_document_from_wrong('criticarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 3, 26, 31, 43, 116, 199, 205, 228, 232}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_documents_union(words):\n",
    "    docs = set()\n",
    "    for word in words.split(' '):\n",
    "        bad_text = [[token.lower() for token in WordPunctTokenizer().tokenize(texto)] for texto in textos]\n",
    "        for i, text in enumerate(bad_text):\n",
    "            if word in text:\n",
    "                docs.add(i)\n",
    "    return docs\n",
    "\n",
    "get_documents_union('crítrica criticar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 52,\n",
       " 62,\n",
       " 66,\n",
       " 105,\n",
       " 110,\n",
       " 176,\n",
       " 183,\n",
       " 186,\n",
       " 187,\n",
       " 190,\n",
       " 196,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 206,\n",
       " 207,\n",
       " 209,\n",
       " 210,\n",
       " 213,\n",
       " 216,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 230,\n",
       " 232,\n",
       " 235,\n",
       " 236,\n",
       " 239,\n",
       " 241}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_documents_from_clean(word):\n",
    "    return set(get_indice_invertido().get(word, []))\n",
    "\n",
    "get_documents_from_clean('crític')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dif_clean_from_class(word, class_words):\n",
    "    return len(get_documents_from_clean(word))-len(get_documents_union(class_words))\n",
    "\n",
    "get_dif_clean_from_class('critic', 'crítrica criticar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
